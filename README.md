## Sequence models
This project looks at natural language processing (NLP). In the 1st part, we implement a CBOW (continuous bag of words) model to learn word embeddings. In the 2nd part, we try to predict a subset of the data using the embeddings from part 1, where we design several sequential models, from multi-head attention (manually implemented), to various RNN architectures. In part 3, we try to predict the target word given the context before it, and we use various RNN architectures for this, such as GRU (gated recurrent unit) and LSTM (long short-term memory) models. We also implement the beam search algorithm to be able to generate multi-length sentences.